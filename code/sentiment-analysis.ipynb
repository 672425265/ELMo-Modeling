{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Deep Contextual Word Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output has been removed intentionally as ELMo is compute intensive. Training was carried on a GPU-accelerated environment.\n",
    "Model requires downloading ELMo representations performed in the second cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import html\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import sklearn.metrics as sklm\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating ELMo(Embeddings from Language Models)\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing ELMo\n",
    "embeddings = elmo_model(\n",
    "    [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
    "    signature=\"default\",\n",
    "    as_dict=True\n",
    ")[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = sess.run(embeddings)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular Expressions\n",
    "whitespace_re = re.compile('\\s+')\n",
    "word_re = re.compile('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation begins from here\n",
    "#Not removing stopwords to encompass semantics and polysemy using ELMo model\n",
    "x = list()\n",
    "y = list()\n",
    "with open('amazon_reviews.txt', 'r',encoding='utf-8') as fr:\n",
    "    for i, line in enumerate(fr):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        classification, line = whitespace_re.split(line, maxsplit=1)\n",
    "        line = line.lower()\n",
    "        line = line.strip()\n",
    "        line = html.unescape(line)\n",
    "        line = ' '.join(word_re.findall(line))\n",
    "        #line = pattern.sub('', line)\n",
    "        x.append(line)\n",
    "        y.append(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binarizing Labels\n",
    "y = list(map(int, y))\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "y = lb.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always a good practice to validate code, therefore checking Y and X\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Max sentence length in order to enable padding\n",
    "max_sentence_length = 0\n",
    "for line in x:\n",
    "    max_sentence_length = max(max_sentence_length, len(line.split()))\n",
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding Function Definition\n",
    "def pad(e, sentence_length=max_sentence_length):\n",
    "    # https://stackoverflow.com/questions/35751306/python-how-to-pad-numpy-array-with-zeros\n",
    "    num_sentences, old_sentence_length, embedding_length = e.shape\n",
    "    e2 = np.zeros((num_sentences, sentence_length, embedding_length))\n",
    "    e2[:, :old_sentence_length, :] = e\n",
    "    return e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings for input data using ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_embeddings = list()\n",
    "step = 500\n",
    "for i in range(int(len(x)/step)+1):\n",
    "    left = i*step\n",
    "    right = (i+1)*step\n",
    "    this_x = x[left:right]\n",
    "\n",
    "    # due to the +1 in the range(...+1), we can end up\n",
    "    # with an empty row at the end. just skip it.\n",
    "    if not this_x:\n",
    "        continue\n",
    "\n",
    "    embeddings = elmo_model(\n",
    "        this_x,\n",
    "        signature=\"default\",\n",
    "        as_dict=True\n",
    "    )[\"elmo\"]\n",
    "    e = sess.run(embeddings)\n",
    "    e = pad(e)\n",
    "    all_embeddings.append(e)\n",
    "all_embeddings = np.concatenate(all_embeddings)\n",
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = all_embeddings.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom implementation of Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "        self.confusion = []\n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        score = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        predict = np.round(np.asarray(self.model.predict(self.validation_data[0])))\n",
    "        targ = self.validation_data[1]\n",
    "\n",
    "        self.val_f1s.append(sklm.f1_score(targ, predict, average='micro'))\n",
    "        self.confusion.append(sklm.confusion_matrix(targ.argmax(axis=1),predict.argmax(axis=1)))\n",
    "        print('F1: %s confusion:\\n%s' % (self.val_f1s[-1], self.confusion[-1]))\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_embeddings,\n",
    "    y[:all_embeddings.shape[0]],\n",
    "    test_size=.01,\n",
    "    stratify=y.argmax(axis=1)[:all_embeddings.shape[0]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/layers/core/\n",
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(\n",
    "    100,\n",
    "    input_shape=(max_sentence_length,embedding_vector_length),\n",
    "))\n",
    "model.add(keras.layers.Dense(\n",
    "    num_classes,\n",
    "    # Hands-On Machine Learning with Scikit-Learn and TensorFlow, ed. 1 by Aurélien Géron\n",
    "    activation='softmax',\n",
    "))\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=False)\n",
    "model.compile(\n",
    "    # https://www.liip.ch/en/blog/sentiment-detection-with-keras-word-embeddings-and-lstm-deep-learning-networks\n",
    "    #loss='categorical_crossentropy',\n",
    "    loss = 'binary_crossentropy',\n",
    "    #optimizer='adam',\n",
    "    optimizer = sgd,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Model training with batch size and epochs\n",
    "model.fit(\n",
    "    #x=all_embeddings,\n",
    "    #y=y[:all_embeddings.shape[0]],\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    #epochs=10,\n",
    "#    callbacks=[metrics],\n",
    "#     verbose=1,\n",
    "#     callbacks=None,\n",
    "#     validation_split= 0.10,\n",
    "#     validation_data=None,\n",
    "#     shuffle=True,\n",
    "#     class_weight=None,\n",
    "#     sample_weight=None,\n",
    "#     initial_epoch=0,\n",
    "#     steps_per_epoch=None,\n",
    "#     validation_steps=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data\n",
    "x_test = list()\n",
    "\n",
    "with open('clean_test.txt', 'r', encoding='utf-8') as fr:\n",
    "    for i, line in enumerate(fr):\n",
    "        classification, line = whitespace_re.split(line, maxsplit=1)\n",
    "        line = line.strip()\n",
    "        line = html.unescape(line)\n",
    "        line = ' '.join(word_re.findall(line))\n",
    "        #line = pattern.sub('', line)\n",
    "        x_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Test Embeddings\n",
    "test_embeddings = list()\n",
    "step = 500\n",
    "for i in range(int(len(x_test)/step)+1):\n",
    "    left = i*step\n",
    "    right = (i+1)*step\n",
    "    this_xtest = x_test[left:right]\n",
    "\n",
    "    # due to the +1 in the range(...+1), we can end up\n",
    "    # with an empty row at the end. just skip it.\n",
    "    if not this_xtest:\n",
    "        continue\n",
    "\n",
    "    embeddings = elmo_model(\n",
    "        this_xtest,\n",
    "        signature=\"default\",\n",
    "        as_dict=True\n",
    "    )[\"elmo\"]\n",
    "    e_test = sess.run(embeddings)\n",
    "    e_test = pad(e_test)\n",
    "    test_embeddings.append(e_test)\n",
    "test_embeddings = np.concatenate(test_embeddings)\n",
    "test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Prediction File\n",
    "with open(\"prediction.txt\",\"w\") as file:\n",
    "    for i in y_classes:\n",
    "        file.write(str(i+1)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
